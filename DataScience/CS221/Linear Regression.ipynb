{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechine Learning Definition\n",
    "---\n",
    "_\"Field of study that gives computers the ability to learn without beging expilicty programmed.\"_ <br>~Arthur Samuel (1959)<br>\n",
    "_\"A computer program is said to learn from experience E with respect to some task T and some preformance measure P, if its preformence on T, as measured by P, improves with experiece E.\"_ <br> ~Tom Mitchell (1998)\n",
    "<br>\n",
    "### Supervised Learning\n",
    "Given the _\"right answers\"_ for each example in the data \n",
    "__Regression:__ Predict continuous valued output\n",
    "__Classification:__ Discrete valued output (0 or 1)\n",
    "<br>\n",
    "### Unsupervised Learning \n",
    "here is dataset can you finde some structure ex.(break same data type into two clusters) \n",
    "__Clustering__ used to  Organize Computing clusteres, Social network analysis, Market segmentation, Astronomical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "---\n",
    "##### Hypothesis:\n",
    "$$h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x^{(i)}$$\n",
    "##### Taxonomy:\n",
    "Training Set -> Learning Algorithm -> hypothesis(x) -> output: y\n",
    "##### Notation:\n",
    "m = Number of training examples<br>\n",
    "n = Number of features<br>\n",
    "x´s = \"input\" variable/features<br>\n",
    "y´s = \"output\" variable/feaures<br>\n",
    "$x^{(i)}$ = value x indexed with<br>\n",
    "$x^{(i)}$ = input (features) of $i^{th}$ trainig example<br>\n",
    "$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Cost function\n",
    "---\n",
    "##### Idea:\n",
    "Choose $\\theta_0$,$\\theta_1$ so that $h_\\theta (x)$ is close to $y$ for our traning examples $(x,y)$\n",
    "<br><br>\n",
    "##### Goal: \n",
    "$\\binom{minimize}{\\theta_0,\\theta_1}$ $J(\\theta_0,\\theta_1)$ => Squared error Cost function\n",
    "<br><br>\n",
    "##### Algorithm:\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum\\limits_{i=1}^m(h\\theta_0(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$$h_\\theta(x)$$__\n",
    "(for fixes $\\theta_1$, this is a function of $x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16427/2249375658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "points = [[1,1],[2,2],[3,3]]\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(0,5,100)\n",
    "plt.plot(x,x*1, \"-r\", label=\"y = 1x\") # best fit\n",
    "plt.plot(x,x*0.5, \"-b\", label=\"y = 0.5x\")\n",
    "plt.plot(x,x*0,\"-y\",label=\"y = 0x\")\n",
    "for lt in points:\n",
    "    for x in lt:\n",
    "        plt.scatter(points[x],points[x])\n",
    "plt.xlabel(\"x\", color=\"r\")\n",
    "plt.ylabel(\"y\", color=\"r\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$$J(\\theta_1)$$__\n",
    "(function of the parameter $\\theta_1$) so define the slope value in this example and slope is calulatet as you know from the school <br>\n",
    "corresponds to value of $\\theta_1 => y = x^2 + 1$ for the minimalization, so the best is 1 since it is the lowest point of parabola \n",
    "<br>$\\binom{minimize}{\\theta_1}$ $J(\\theta_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "---\n",
    "##### Def:\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of __steepest descent__ as __defined__ by the negative of the __gradient__. <br>\n",
    "• Need to choose $\\alpha$ <br>\n",
    "• Needs many iterations <br>\n",
    "• Workd well even when $n$ is large e.g $n = 10^6$\n",
    "<br><br>\n",
    "##### Idea:\n",
    "Init with some $\\theta_0, \\theta_1$ and keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ till return the minimum.\n",
    "<br><br>\n",
    "##### Goal: \n",
    "Return the $\\binom{min}{\\theta_0,\\theta_1} J(\\theta_0,\\theta_1)$ thus find the __Local Optimum__\n",
    "<br><br>\n",
    "##### Algorithm\n",
    "$\\color{purple}{def}$repeat until convergence{ \n",
    "\n",
    "$$\\theta_j := \\theta_j-\\alpha\\frac{\\delta}{\\delta\\theta_j} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}\\color{green}{J(\\theta_0,\\theta_1)} = \\frac{\\delta}{\\delta\\theta_j}*\\color{green}{\\frac{1}{2m}\\sum_\\limits{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\theta_0 := \\theta_0-\\alpha \\color{yellow}{\\frac{1}{m}}\\color{#2180ac}{\\sum_\\limits{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}}) * x^{(i)}$$\n",
    "\n",
    "$$\\color{yellow}{\\frac{\\delta}{\\delta\\theta_1}} \\color{#2180ac}{J(\\theta_0,\\theta_1)} * x^{(i)}$$\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    " } (for j = 0 and j = 1) <=> __Simultaneously__ update $\\theta_0$ and $\\theta_1$ // blue = slope; green = minimalize; $\\alpha :=$ __learning rate__ step sizes<; on yellow you can cross out the $\\delta$; IMPORTANT yellow and blue denote one vector\n",
    "<br>\n",
    "For fixes $\\alpha$ rate: As approach to local minimum, gradient descent steps become smaller automatically, so no need to decrease $\\alpha$ over time\n",
    "<br><br>\n",
    "##### Vectorized the Equation\n",
    "<img src=\"pic/GD Overview.png\">\n",
    "<br><br>\n",
    "##### \"Batch\" \n",
    "Each step of gradient descent uses al lthe training examples, thus the sum of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(2,4),(4,2)]\n",
    "def F(w):\n",
    "   return sum((x * x - y)**2 for x, y in points)\n",
    "def dF(w):\n",
    "   return sum(2*(x*y-y)*x for x, y in points)\n",
    "# Gradient descent\n",
    "w = 0\n",
    "eta = 0.01\n",
    "for t in range(100):\n",
    "   value = F(w)\n",
    "   gradient = dF(w)\n",
    "   w = w -eta * gradient\n",
    "   print(\"iteration {}: w = {}, F(w) = {}\".format(t,w,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "---\n",
    "##### Hypothesis:\n",
    "$$h_\\theta(x) = \\theta_0+\\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n => \\theta^Tx$$\n",
    "<br><br>\n",
    "##### Algorithm:\n",
    "Repeat{$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_\\limits{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})x_j^{(i)}$$\n",
    "} simultaneously update $\\theta_j$ for j = 0,...,n)\n",
    "<br><br>\n",
    "##### Features Scaling:  \n",
    "##### Mean normalization:\n",
    "Replace $x_i$ with $x_i-\\mu_i$ to make features have approximately zero mean (Do not apply to $x_0 = 1$)<br>\n",
    "$x_1 \\leftarrow \\frac{x_1-\\mu_1}{s_1}$; where $\\mu$ is avg. value of x in traning set; $s$ is ragne, so (max-min)\n",
    "<br>\n",
    "$x_1 = \\frac{x_1-1000}{2000}$<br>\n",
    "$x_2 = \\frac{\\text{x_2} - 2}{5}$<br>\n",
    "$\\therefore -0.5 <= x_1 <= 0.5; -0.5 <= x_2 <= 0.5$<br>\n",
    "Gradient work correctly if after each iter of $\\binom{min}{\\theta}$J($\\theta$) the __value should decrease__ if not you should use lower __learning rate $\\alpha$__;<br>\n",
    "Threshold of iter vary for each case, so Do automatic convergence test function to ensure.<br>\n",
    "For sufficently small $\\alpha$, the $J(\\theta)$ should decrease on each iteration<br>\n",
    "• If $\\alpha$ is too small, then slow convergence<br>\n",
    "• If $\\alpha$ is too large, then $J(\\theta)$ may not decrease on each iternation; may not converge. <br>\n",
    "• Choose $\\alpha$ as $3\\times$ multiplier<br>\n",
    "__Debug:__ ploting that $J(\\theta)$ as the number of Interation.<br>\n",
    "<br>\n",
    "\n",
    "##### Polynomial Regression\n",
    "• Try to combinate the adquate features into one.\n",
    "• Use vary of function in the equation<br>\n",
    "$h_\\theta(x) = \\theta_0+\\theta_1(x)+\\theta(x)^2 $\n",
    "<br>\n",
    "$ h_\\theta(x) =\\theta_0+\\theta_1(x)+\\theta_2\\sqrt{(x)}$ \n",
    "<br>etc.<br>\n",
    "\n",
    "##### Normal Equation _(for non-linear problem, use GD)_\n",
    "Method to sove for $\\theta$ analytically. <br>\n",
    "• No need to choose $\\alpha$ <br>\n",
    "• Do not need to iterat <br>   \n",
    "• Slow if $n$ is very large e.g after n=10000 <br>\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$<br>\n",
    "\n",
    "$m$ __examples__ $(x^{1},y^{1},...,(x^{(m)},y^{m})$ <br>while $n$ __features__\n",
    "where: <br>X is $m\\times(n+1)$ matrix, so X =\\begin{bmatrix}\n",
    "-& (x^{1})^T &-\\\\\n",
    "-& (x^{2})^T &-\\\\\n",
    "-& (x^{m})^T &-\\\\\n",
    "\\end{bmatrix} <br>\n",
    "<br> \n",
    "If $X^T X$ is non-invertible, then caused by:<br>\n",
    "• Redundant features (linearly dependent)\n",
    "    - e.g. x_1 = size in feet^2\n",
    "           x_2 = size in m^2\n",
    "• Too many features (e.g. $m <= n$)\n",
    "    - Deltete some features, or use regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
